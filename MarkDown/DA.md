##### 1. 决策树

通过以往经验的总结对后续问题结果的演算

主要构成：根节点、子节点、叶节点

###### 1. 构建

1. 根节点属性的选择。
2. 子节点属性的选择
3. 何时停止取得目标状态叶节点

###### 2. 裁剪

如果去掉多余的属性判断同样可以得到较好的结果，可以进行剪枝。

过多的属性判断或者样本数量较少都可能产生过拟合，导致模型的泛化能力差。

构建决策树主要基于纯度，纯度越高则信息熵越低，反之亦然

1. 纯度：尽量使目标变量的分歧最小

   + 信息增益（ID3算法）

     + 指的是由划分可以带来纯度的提高信息熵的减少，由父亲节点的信息熵减去所有子节点的信息熵

     + $$
       (增益)Gain(D,a) = Entropy(D) - \sum_{i=1}^{k}(\frac{|Di|}{|D|})Entropy(Di)
       $$ { Di/D 代表子节点占父节点的次数,假如D为天气，D1为晴(一次去，两次不去)，D2为阴(三次去两次不去),D3为小雨(两次不去，一次去)}

   + 公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。

   + 缺点，一定概率下会将对任务分类没有太多作用的属性选为最有属性
     
   + 信息增益率（C4.5）

     + 由信息增益比上分裂信息度量(指一个属性分裂的广度和均匀)

     + $$
       (分裂信息度量)SplitInfoMation(S,A)=-\sum_{i=1}^{c}(\frac{Si}{S})log_2(\frac{Si}{S})
       $$ {n个样例被属性A分割为c个子集}

   + 基尼指数（Cart算法）

2. 信息熵：代表了信息的不确定度

   + $$
     (熵)Entropy(t) = -\sum_{i=0}^{c-1} P(i|t)log_2P(i|t)
     $$

   + P(i|t) 代表节点 t 为 i 的概率
   + 如：六次决策，一次成功，五次失败
     
     + Entropy = - ( 1/6 ) * log<sub>2</sub> (1/6) -  ( 5/6 ) * log<sub>2</sub> (5/6) = 0.65













